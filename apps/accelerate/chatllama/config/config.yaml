---
trainer_config:
  # learning rates
  actor_lr: 0.00001
  critic_lr: 0.00001
  # PPO Hyperparameters
  actor_eps_clip: 0.2
  critic_eps_clip: 0.2
  beta_s: 0.1
  # path to examples to be sampled (training dataset) see rlhf_dataset.json
  examples_path: "path-to-json-of-examples"
  # number of episodes and generation performed for each episode
  # in the train() method
  num_episodes: 128
  max_timesteps: 4
  # number of timesteps after which the learn() method is called 
  # (to update the weights)
  update_timesteps: 4
  # number of example sampled at each timestep
  num_examples: 32
  # batch and epochs for the training
  batch_size: 32
  epochs: 8
  # number of learning steps (i.e. learn()) after which a checkpoint is saved
  update_checkpoint: 8
  checkpoint_folder: "./models/checkpoints"

actor_config:
  model: "llama-7B" 
  model_path: "path-to-model"
  checkpoint_folder: "path-to-checkpoint-folder"
  tokenizer_folder: "path-to-tokenizer"
  train_dataset_path: "path-to-training-dataset"
  validation_dataset_path: null
  # froze model embedding during training
  froze_embeddings: True
  # use fairscale layers to build the model instead of vanilla pytorch
  use_fairscale: False
  # max sequence length for the actor (i.e. prompt + completion)
  max_sequence_length: 1024
  # max tokens generated by the actor (completion only)
  max_tokens: 512
  # temperature for the actor
  temperature: 0.9
  batch_size: 32
  # number iteration after print
  iteration_per_print: 1
  lr: 0.0001
  epochs: 32
  # deepspeed settings
  deepspeed_enable: False
  deepspeed_config_path: "path-to-deepspeed-conf"

reward_config:
  # model to be chosen are gp2-large, bart-base, longformer-base-4096
  # more can be simply added in the reward.py __init__()
  model: "gpt2-large"
  # hidden size of the additional ffw head to produce the scores
  model_head_hidden_size: 2048
  model_folder: "./models"
  train_dataset_path: "path-to-training-dataset"
  validation_dataset_path: null
  batch_size: 64
  epochs: 32
  iteration_per_print: 1
  lr: 0.0001
  # llm parameters for dataset generation
  # disable llm if you dont have an API key
  llm_enable: True
  llm_model: "text-davinci-003"
  llm_temperature: 0.5
  llm_max_tokens: 1024
  # deepspeed settings
  deepspeed_enable: False
  deepspeed_config_path: "path-to-deepspeed-conf"

critic_config:
  # model to be chosen are gp2-large, bart-base, longformer-base-4096
  # more can be simply added in the reward.py __init__()
  model: "gpt2-large"
  # hidden size of the additional ffw head to produce the scores
  model_head_hidden_size: 2048
  model_folder: "./models"
  # deepspeed settings
  deepspeed_enable: False
  deepspeed_config_path: "path-to-deepspeed-conf"